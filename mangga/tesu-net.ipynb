{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_images_path = 'dataset/train'\n",
    "train_annotations_path = 'dataset/train/_annotations.coco.json'\n",
    "valid_images_path = 'dataset/valid'\n",
    "valid_annotations_path = 'dataset/valid/_annotations.coco.json'\n",
    "test_images_path = 'dataset/test'\n",
    "test_annotations_path = 'dataset/test/_annotations.coco.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function to yield batches of data\n",
    "def coco_data_generator(images_path, annotations_path, batch_size=32):\n",
    "    coco = COCO(annotations_path)\n",
    "    image_ids = coco.getImgIds()\n",
    "    np.random.shuffle(image_ids)\n",
    "    \n",
    "    while True:\n",
    "        for start in range(0, len(image_ids), batch_size):\n",
    "            end = min(start + batch_size, len(image_ids))\n",
    "            batch_image_ids = image_ids[start:end]\n",
    "            images = []\n",
    "            masks = []\n",
    "            \n",
    "            for img_id in batch_image_ids:\n",
    "                img_info = coco.loadImgs(img_id)[0]\n",
    "                img_path = os.path.join(images_path, img_info['file_name'])\n",
    "                image = cv2.imread(img_path)\n",
    "                \n",
    "                if image is None:\n",
    "                    print(f\"Image not found at path: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                images.append(image / 255.0)\n",
    "                \n",
    "                ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "                anns = coco.loadAnns(ann_ids)\n",
    "                \n",
    "                if not anns:\n",
    "                    print(f\"No annotations found for image ID: {img_id}\")\n",
    "                    continue\n",
    "                \n",
    "                mask = np.zeros((img_info['height'], img_info['width'], 1))\n",
    "                \n",
    "                for ann in anns:\n",
    "                    if len(ann['segmentation']) == 0:\n",
    "                        print(f\"Skipping annotation {ann['id']} for image ID {img_id} due to empty segmentation\")\n",
    "                        continue\n",
    "                    try:\n",
    "                        ann_mask = coco.annToMask(ann)\n",
    "                        mask = np.maximum(mask, np.expand_dims(ann_mask, axis=-1))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing annotation {ann['id']} for image ID {img_id}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                masks.append(mask)\n",
    "            \n",
    "            yield np.array(images), np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the U-Net model (simplified version)\n",
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = tf.keras.Input(input_size)\n",
    "    conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "\n",
    "    up1 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv3)\n",
    "    up1 = tf.keras.layers.concatenate([conv2, up1], axis=3)\n",
    "    conv4 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(up1)\n",
    "    conv4 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "\n",
    "    up2 = tf.keras.layers.UpSampling2D(size=(2, 2))(conv4)\n",
    "    up2 = tf.keras.layers.concatenate([conv1, up2], axis=3)\n",
    "    conv5 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(up2)\n",
    "    conv5 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    conv6 = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(conv5)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[conv6])\n",
    "    return model\n",
    "\n",
    "model = unet_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Skipping annotation 2343 for image ID 2339 due to empty segmentation\n",
      "Skipping annotation 2165 for image ID 2161 due to empty segmentation\n",
      "Skipping annotation 2701 for image ID 2697 due to empty segmentation\n",
      "Skipping annotation 2766 for image ID 2762 due to empty segmentation\n",
      "Skipping annotation 580 for image ID 580 due to empty segmentation\n",
      "Skipping annotation 625 for image ID 624 due to empty segmentation\n",
      "Skipping annotation 1214 for image ID 1211 due to empty segmentation\n",
      "Skipping annotation 1092 for image ID 1089 due to empty segmentation\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 25.0 MiB for an array with shape (8, 640, 640, 1) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m coco_data_generator(train_images_path, train_annotations_path, batch_size)\n\u001b[0;32m      8\u001b[0m valid_generator \u001b[38;5;241m=\u001b[39m coco_data_generator(valid_images_path, valid_annotations_path, batch_size)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate on test data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m coco_data_generator(test_images_path, test_annotations_path, batch_size)\n",
      "File \u001b[1;32mc:\\Users\\ferra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[34], line 47\u001b[0m, in \u001b[0;36mcoco_data_generator\u001b[1;34m(images_path, annotations_path, batch_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     masks\u001b[38;5;241m.\u001b[39mappend(mask)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images), np\u001b[38;5;241m.\u001b[39marray(masks)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 25.0 MiB for an array with shape (8, 640, 640, 1) and data type float64"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "batch_size = 8\n",
    "steps_per_epoch = len(COCO(train_annotations_path).getImgIds()) // batch_size\n",
    "validation_steps = len(COCO(valid_annotations_path).getImgIds()) // batch_size\n",
    "\n",
    "# Train the model\n",
    "train_generator = coco_data_generator(train_images_path, train_annotations_path, batch_size)\n",
    "valid_generator = coco_data_generator(valid_images_path, valid_annotations_path, batch_size)\n",
    "\n",
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, validation_data=valid_generator, validation_steps=validation_steps)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_generator = coco_data_generator(test_images_path, test_annotations_path, batch_size)\n",
    "test_steps = len(COCO(test_annotations_path).getImgIds()) // batch_size\n",
    "\n",
    "model.evaluate(test_generator, steps=test_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
